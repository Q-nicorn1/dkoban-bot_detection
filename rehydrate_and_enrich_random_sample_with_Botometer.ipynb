{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rehydrate and enrich a random set of user accounts with Botometer and BotometerLite scores\n",
    "\n",
    "1. Make an output directory to write API results to and load libraries\n",
    "2. Randomly sample n tweets [GWU tweet sets](https://tweetsets.library.gwu.edu/datasets)\n",
    "3. Rehydrate the tweet IDs in the Python terminal using `twarc`\n",
    "4. Sample n unique, english speaking accounts\n",
    "5. Enrich with Botometer scores\n",
    "6. Enrich with BotometerLite scores\n",
    "7. Merge results and output to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make an output directory to write API results to and load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from twarc import Twarc\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import random \n",
    "import twitter_col\n",
    "import botometer\n",
    "\n",
    "# make an output directory to write API responses to\n",
    "collection_name = 'covid_bot_scores'\n",
    "if not os.path.exists(collection_name):\n",
    "    os.makedirs(collection_name)\n",
    "    os.makedirs(collection_name + '/botometer/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Randomly sample n tweets [GWU tweet sets](https://tweetsets.library.gwu.edu/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "# Randomly select n tweet IDs from each of the n files\n",
    "files = os.listdir('COVID_version7/')\n",
    "files = files[0:5]\n",
    "\n",
    "# set seed = 1 for reproducibility and randomly sample n accounts\n",
    "random.seed(1)\n",
    "tweet_ids = []\n",
    "for file in files:\n",
    "    with open('COVID_version7/' + file) as f:\n",
    "        temp = [line.rstrip() for line in f]\n",
    "        temp = random.sample(temp, 500)\n",
    "        tweet_ids = tweet_ids + temp\n",
    "        print(len(tweet_ids))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the tweet ids to a text file\n",
    "with open(collection_name + '/sample_tweet_ids.txt', 'w') as f:\n",
    "    for item in tweet_ids:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Rehydrate the tweet IDs in the Python terminal using `twarc`\n",
    "\n",
    "1. twarc configure\n",
    "2. twarc hydrate COVID_sample_tweet_ids.txt > COVID_sample_rehydrated.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample n unique, english speaking accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |                   #                           | 20068 Elapsed Time: 0:00:01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = twitter_col.parse_twitter_json(\"/Users/dankoban/Documents/EM6574/COVID_sample_rehydrated.jsonl\", \n",
    "                                    to_csv = False, sentiment = False)\n",
    "\n",
    "tweets = tweets[tweets['status_lang'] == 'en']\n",
    "\n",
    "random.seed(1)\n",
    "user_ids = tweets['id_str'].unique().tolist()\n",
    "user_ids = random.sample(user_ids, 10)\n",
    "len(user_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enrich with Botometer scores\n",
    "\n",
    "To run this code you will need a Twitter developer account and Rapid API key. Each API result is written to an individual csv file to prevent losing results in the event a kernel dies. Botometer allows user to check up to 17,280 Twitter accounts per day. However, I have never come close to reaching that limit due to latency of the API. It generally takes me about 2 days to pull 10,000 accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botometer\n",
    "import time\n",
    "\n",
    "rapidapi_key = ''\n",
    "twitter_app_auth = {\n",
    "    'consumer_key': '',\n",
    "    'consumer_secret': '',\n",
    "    'access_token': '',\n",
    "    'access_token_secret': '',\n",
    "  }\n",
    "bom = botometer.Botometer(wait_on_ratelimit=True,\n",
    "                          rapidapi_key=rapidapi_key,\n",
    "                          **twitter_app_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "botometer_full = []\n",
    "i = 0\n",
    "for user in user_ids:\n",
    "    i+=1\n",
    "    try:\n",
    "        result = bom.check_account(user)            \n",
    "        temp = pd.DataFrame(result)\n",
    "        temp = pd.DataFrame({'id_str': [temp['user']['user_data']['id_str']],\n",
    "                             'screen_name': [temp['user']['user_data']['screen_name']],\n",
    "                             'cap_en': [temp['cap']['english']],\n",
    "                             'cap_un': [temp['cap']['universal']],\n",
    "\n",
    "                             'astroturf_raw_en': [temp['raw_scores']['english']['astroturf']],\n",
    "                             'fake_follower_raw_en': [temp['raw_scores']['english']['fake_follower']],\n",
    "                             'financial_raw_en': [temp['raw_scores']['english']['financial']],\n",
    "                             'other_raw_en': [temp['raw_scores']['english']['other']],\n",
    "                             'overall_raw_en': [temp['raw_scores']['english']['overall']],\n",
    "                             'self_declared_raw_en': [temp['raw_scores']['english']['self_declared']],\n",
    "                             'spammer_raw_en': [temp['raw_scores']['english']['spammer']],\n",
    "\n",
    "                             'astroturf_display_en': [temp['display_scores']['english']['astroturf']],\n",
    "                             'fake_follower_display_en': [temp['display_scores']['english']['fake_follower']],\n",
    "                             'financial_display_en': [temp['display_scores']['english']['financial']],\n",
    "                             'other_display_en': [temp['display_scores']['english']['other']],\n",
    "                             'overall_display_en': [temp['display_scores']['english']['overall']],\n",
    "                             'self_declared_display_en': [temp['display_scores']['english']['self_declared']],\n",
    "                             'spammer_display_en': [temp['display_scores']['english']['spammer']],\n",
    "\n",
    "                             'astroturf_raw_un': [temp['raw_scores']['universal']['astroturf']],\n",
    "                             'fake_follower_raw_un': [temp['raw_scores']['universal']['fake_follower']],\n",
    "                             'financial_raw_un': [temp['raw_scores']['universal']['financial']],\n",
    "                             'other_raw_un': [temp['raw_scores']['universal']['other']],\n",
    "                             'overall_raw_un': [temp['raw_scores']['universal']['overall']],\n",
    "                             'self_declared_raw_un': [temp['raw_scores']['universal']['self_declared']],\n",
    "                             'spammer_raw_un': [temp['raw_scores']['universal']['spammer']],\n",
    "\n",
    "                             'astroturf_display_un': [temp['display_scores']['universal']['astroturf']],\n",
    "                             'fake_follower_display_un': [temp['display_scores']['universal']['fake_follower']],\n",
    "                             'financial_display_un': [temp['display_scores']['universal']['financial']],\n",
    "                             'other_display_un': [temp['display_scores']['universal']['other']],\n",
    "                             'overall_display_un': [temp['display_scores']['universal']['overall']],\n",
    "                             'self_declared_display_un': [temp['display_scores']['universal']['self_declared']],\n",
    "                             'spammer_display_un': [temp['display_scores']['universal']['spammer']]\n",
    "                     })\n",
    "        print(i)\n",
    "        timestr = time.strftime(\"%m%d%Y_%H%M\")\n",
    "        temp.to_csv(collection_name + '/botometer/' + str(user) + timestr + \".csv\")\n",
    "        botometer_full.append(temp)\n",
    "\n",
    "    except:\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enrich with BotometerLite scores \n",
    "\n",
    "BotometerLite allows for batch queries of up to 20,000 accounts per day and completes in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function to split user ids into batches.  \n",
    "# BotometerLite accepts up to 100 user IDs per query.\n",
    "def batch(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]   \n",
    "\n",
    "# Read in the Botometer scores gathered in step 3.\n",
    "files = os.listdir(collection_name + '/botometer/')\n",
    "df_list = []\n",
    "for file in files:\n",
    "    temp = pd.read_csv(collection_name + '/botometer/' + file, \n",
    "                        dtype={'id_str': 'str'})\n",
    "    df_list.append(temp)\n",
    "df = pd.concat(df_list)\n",
    "        \n",
    "user_ids = df['id_str'].unique().tolist()                \n",
    "batches = list(batch(user_ids, 100))\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blt_twitter = botometer.BotometerLite(rapidapi_key=rapidapi_key, **twitter_app_auth)\n",
    "blt_scores = []\n",
    "for batch in batches:\n",
    "    temp = blt_twitter.check_accounts_from_user_ids(batch)\n",
    "    blt_scores.append(pd.DataFrame(temp))\n",
    "blt_scores = pd.concat(blt_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combine Botometer and BotometerLite scores into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "blt_scores = blt_scores.rename(columns={'user_id': 'id_str',\n",
    "                                        'botscore': 'bot_lite'})\n",
    "blt_scores['id_str'] = blt_scores['id_str'].astype('str')\n",
    "\n",
    "# merge bot scores\n",
    "merged_bot_scores = df.merge(blt_scores[['id_str', 'bot_lite']], how = 'left', on = 'id_str')\n",
    "\n",
    "keep_cols = ['id_str', 'cap_en', 'cap_un',\n",
    "       'astroturf_raw_en', 'fake_follower_raw_en', 'financial_raw_en',\n",
    "       'other_raw_en', 'overall_raw_en', 'self_declared_raw_en',\n",
    "       'spammer_raw_en', 'astroturf_display_en', 'fake_follower_display_en',\n",
    "       'financial_display_en', 'other_display_en', 'overall_display_en',\n",
    "       'self_declared_display_en', 'spammer_display_en', 'astroturf_raw_un',\n",
    "       'fake_follower_raw_un', 'financial_raw_un', 'other_raw_un',\n",
    "       'overall_raw_un', 'self_declared_raw_un', 'spammer_raw_un',\n",
    "       'astroturf_display_un', 'fake_follower_display_un',\n",
    "       'financial_display_un', 'other_display_un', 'overall_display_un',\n",
    "       'self_declared_display_un', 'spammer_display_un', 'bot_lite']\n",
    "merged_bot_scores = merged_bot_scores[keep_cols]\n",
    "merged_bot_scores = merged_bot_scores[merged_bot_scores['bot_lite'].notnull()]\n",
    "merged_bot_scores\n",
    "\n",
    "# merge with the user profile info\n",
    "tweets_filtered = tweets[tweets['id_str'].isin(merged_bot_scores.id_str)]\n",
    "final_df = tweets_filtered.merge(merged_bot_scores, how = 'left', on = 'id_str')\n",
    "final_df.to_csv(collection_name + '/enriched_accounts.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forcenexus",
   "language": "python",
   "name": "forcenexus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
