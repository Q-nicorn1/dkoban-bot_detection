{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rehydrate and enrich a random set of user accounts with Botometer and BotometerLite scores\n",
    "\n",
    "1. Make an output directory to write API results to and load libraries\n",
    "2. Randomly sample n tweets [GWU tweet sets](https://tweetsets.library.gwu.edu/datasets)\n",
    "3. Rehydrate the tweet IDs in the Python terminal using `twarc`\n",
    "4. Sample n unique, english speaking accounts\n",
    "5. Enrich with Botometer scores\n",
    "6. Enrich with BotometerLite scores\n",
    "7. Merge results and output to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rehydrate tweet IDs from Bot-wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import twitter_col\n",
    "import os\n",
    "bots = pd.read_json('data/botwiki-2019_tweets.json')\n",
    "bots = pd.json_normalize(bots['user'])\n",
    "bots['id'].to_csv(\"data/bot_wiki_ids.txt\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. twarc configure\n",
    "2. twarc hydrate COVID_sample_tweet_ids.txt > COVID_sample_rehydrated.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read in rehydrated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = twitter_col.parse_twitter_json(\"/Users/dankoban/Documents/bot_detection/data/bot_wiki_rehydrated.jsonl\", \n",
    "                                    to_csv = False, sentiment = False)\n",
    "\n",
    "tweets = tweets[tweets['status_lang'] == 'en']\n",
    "user_ids = tweets['id_str'].unique().tolist()\n",
    "len(user_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enrich with Botometer scores\n",
    "\n",
    "To run this code you will need a Twitter developer account and Rapid API key. Each API result is written to an individual csv file to prevent losing results in the event a kernel dies. Botometer allows user to check up to 17,280 Twitter accounts per day. However, I have never come close to reaching that limit due to latency of the API. It generally takes me about 2 days to pull 10,000 accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botometer\n",
    "import time\n",
    "\n",
    "rapidapi_key = ''\n",
    "twitter_app_auth = {\n",
    "    'consumer_key': '',\n",
    "    'consumer_secret': '',\n",
    "    'access_token': '',\n",
    "    'access_token_secret': '',\n",
    "  }\n",
    "bom = botometer.Botometer(wait_on_ratelimit=True,\n",
    "                          rapidapi_key=rapidapi_key,\n",
    "                          **twitter_app_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botometer_full = []\n",
    "i = 0\n",
    "for user in user_ids:\n",
    "    i+=1\n",
    "    try:\n",
    "        result = bom.check_account(user)            \n",
    "        temp = pd.DataFrame(result)\n",
    "        temp = pd.DataFrame({'id_str': [temp['user']['user_data']['id_str']],\n",
    "                             'screen_name': [temp['user']['user_data']['screen_name']],\n",
    "                             'cap_en': [temp['cap']['english']],\n",
    "                             'cap_un': [temp['cap']['universal']],\n",
    "\n",
    "                             'astroturf_raw_en': [temp['raw_scores']['english']['astroturf']],\n",
    "                             'fake_follower_raw_en': [temp['raw_scores']['english']['fake_follower']],\n",
    "                             'financial_raw_en': [temp['raw_scores']['english']['financial']],\n",
    "                             'other_raw_en': [temp['raw_scores']['english']['other']],\n",
    "                             'overall_raw_en': [temp['raw_scores']['english']['overall']],\n",
    "                             'self_declared_raw_en': [temp['raw_scores']['english']['self_declared']],\n",
    "                             'spammer_raw_en': [temp['raw_scores']['english']['spammer']],\n",
    "\n",
    "                             'astroturf_display_en': [temp['display_scores']['english']['astroturf']],\n",
    "                             'fake_follower_display_en': [temp['display_scores']['english']['fake_follower']],\n",
    "                             'financial_display_en': [temp['display_scores']['english']['financial']],\n",
    "                             'other_display_en': [temp['display_scores']['english']['other']],\n",
    "                             'overall_display_en': [temp['display_scores']['english']['overall']],\n",
    "                             'self_declared_display_en': [temp['display_scores']['english']['self_declared']],\n",
    "                             'spammer_display_en': [temp['display_scores']['english']['spammer']],\n",
    "\n",
    "                             'astroturf_raw_un': [temp['raw_scores']['universal']['astroturf']],\n",
    "                             'fake_follower_raw_un': [temp['raw_scores']['universal']['fake_follower']],\n",
    "                             'financial_raw_un': [temp['raw_scores']['universal']['financial']],\n",
    "                             'other_raw_un': [temp['raw_scores']['universal']['other']],\n",
    "                             'overall_raw_un': [temp['raw_scores']['universal']['overall']],\n",
    "                             'self_declared_raw_un': [temp['raw_scores']['universal']['self_declared']],\n",
    "                             'spammer_raw_un': [temp['raw_scores']['universal']['spammer']],\n",
    "\n",
    "                             'astroturf_display_un': [temp['display_scores']['universal']['astroturf']],\n",
    "                             'fake_follower_display_un': [temp['display_scores']['universal']['fake_follower']],\n",
    "                             'financial_display_un': [temp['display_scores']['universal']['financial']],\n",
    "                             'other_display_un': [temp['display_scores']['universal']['other']],\n",
    "                             'overall_display_un': [temp['display_scores']['universal']['overall']],\n",
    "                             'self_declared_display_un': [temp['display_scores']['universal']['self_declared']],\n",
    "                             'spammer_display_un': [temp['display_scores']['universal']['spammer']]\n",
    "                     })\n",
    "        print(i)\n",
    "        timestr = time.strftime(\"%m%d%Y_%H%M\")\n",
    "        temp.to_csv('/Users/dankoban/Documents/bot_detection/data/' + str(user) + timestr + \".csv\")\n",
    "        botometer_full.append(temp)\n",
    "\n",
    "    except:\n",
    "        pass  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enrich with BotometerLite scores \n",
    "\n",
    "BotometerLite allows for batch queries of up to 20,000 accounts per day and completes in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to split user ids into batches.  \n",
    "# BotometerLite accepts up to 100 user IDs per query.\n",
    "def batch(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]   \n",
    "\n",
    "# Read in the Botometer scores gathered in step 3.\n",
    "files = os.listdir('/Users/dankoban/Documents/bot_detection/data/')\n",
    "df_list = []\n",
    "for file in files:\n",
    "    temp = pd.read_csv('/Users/dankoban/Documents/bot_detection/data/' + file, \n",
    "                        dtype={'id_str': 'str'})\n",
    "    df_list.append(temp)\n",
    "df = pd.concat(df_list)\n",
    "        \n",
    "user_ids = df['id_str'].unique().tolist()                \n",
    "batches = list(batch(user_ids, 100))\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blt_twitter = botometer.BotometerLite(rapidapi_key=rapidapi_key, **twitter_app_auth)\n",
    "blt_scores = []\n",
    "for batch in batches:\n",
    "    temp = blt_twitter.check_accounts_from_user_ids(batch)\n",
    "    blt_scores.append(pd.DataFrame(temp))\n",
    "blt_scores = pd.concat(blt_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine Botometer and BotometerLite scores into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blt_scores = blt_scores.rename(columns={'user_id': 'id_str',\n",
    "                                        'botscore': 'bot_lite'})\n",
    "blt_scores['id_str'] = blt_scores['id_str'].astype('str')\n",
    "\n",
    "# merge bot scores\n",
    "merged_bot_scores = df.merge(blt_scores[['id_str', 'bot_lite']], how = 'left', on = 'id_str')\n",
    "\n",
    "keep_cols = ['id_str', 'cap_en', 'cap_un',\n",
    "       'astroturf_raw_en', 'fake_follower_raw_en', 'financial_raw_en',\n",
    "       'other_raw_en', 'overall_raw_en', 'self_declared_raw_en',\n",
    "       'spammer_raw_en', 'astroturf_display_en', 'fake_follower_display_en',\n",
    "       'financial_display_en', 'other_display_en', 'overall_display_en',\n",
    "       'self_declared_display_en', 'spammer_display_en', 'astroturf_raw_un',\n",
    "       'fake_follower_raw_un', 'financial_raw_un', 'other_raw_un',\n",
    "       'overall_raw_un', 'self_declared_raw_un', 'spammer_raw_un',\n",
    "       'astroturf_display_un', 'fake_follower_display_un',\n",
    "       'financial_display_un', 'other_display_un', 'overall_display_un',\n",
    "       'self_declared_display_un', 'spammer_display_un', 'bot_lite']\n",
    "merged_bot_scores = merged_bot_scores[keep_cols]\n",
    "merged_bot_scores = merged_bot_scores[merged_bot_scores['bot_lite'].notnull()]\n",
    "merged_bot_scores\n",
    "\n",
    "# merge with the user profile info\n",
    "tweets_filtered = tweets[tweets['id_str'].isin(merged_bot_scores.id_str)]\n",
    "final_df = tweets_filtered.merge(merged_bot_scores, how = 'left', on = 'id_str')\n",
    "final_df.to_csv('/Users/dankoban/Documents/bot_detection/data/' + 'enriched_accounts.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forcenexus",
   "language": "python",
   "name": "forcenexus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
